<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> 
</script>

# AI相关数学基础
## 参考内容
[1] 极客时间：《人工智能基础》, 王天一
## 1. 线性代数
**基本概念**
* 集合(set), 由具有共性的对象构成的全体。
* 单个数叫做标量（scalar），一个标量可以是实数或者**复数**，注意，在数学中，复数是最常用的“域”，即满足象代数中“域”定义的集合，实数域是复数域的特例而已。
* 如果多个标量按照一定顺序组成一个序列，就是向量， vector。
* 向量的每个元素又是一个向量的话，就是矩阵，matrix
* 如果将矩阵中的每个标量元素换成向量的话，就得到张量， tensor
* 线性代数式用“虚拟数字”表示真实物理世界的工具。想想“万物皆数”的理念。
* 计算机的量：标量占据零维数组；向量为一维数组，例如语音信号；矩阵是二维数组，例如灰度图像；张量是三维或者更高维数组，例如RGB图像和视频。
* 范数（norm）：单个向量大小的度量，作用是将向量映射到一个非负的数值。通常的$L ^ p$范数的定义为：${\left| {\bf{x}} \right|_p} = {\left( {\sum\limits_i {{{\left| {{x_i}} \right|}^p}} } \right)^{\frac{1}{p}}}$, 所以$L^1$范数就是向量所有元素绝对值的和；$L^2$范数就是向量的长度； $L ^ {\infty}$就是向量中最大元素的取值。范数计算的是单个向量的尺度。
* 接下来考虑两个向量的关系，一般用内积(inner product)。计算表达式为：$\left\langle {{\bf{x,y}}} \right\rangle  = \sum\limits_i {{x_i} \cdot {y_i}}$, 内积表示的是两个向量的夹角。如果<x,y>=0, 则表示夹角为90度，即$\cos \Theta =0$, 即两个向量垂直。在高维空间中，这种关系被称为正交。 如果两个向量正交，则线性无关，相互独立。
* 线性空间：一个包含相同维数的向量集合，并且定义了加法和数乘运算，这样的集合称为线性空间。例如：平时的实数域、复数域、二维、三维空间，都是线性空间。
* 定义了内积运算的线性空间称为内积空间。
* 在线性空间中，任意向量代表的都是n维空间的一个点，反过来，空间中的任一点都可以唯一地用一个向量表示，两者等效。
* 空间中的点和向量的映射中，参考的选择很关键，就是说不通的坐标系（代表不通的基）对应的向量值不同。
* 在内积空间中，一组两两正交的向量构成空间的正交基。如果这些正交基的L-2范数为1，则为标准正交基。定义了正交基，就确定了向量和点之间的对应关系。
* 内积空间的正交基不唯一，例如：二维空间，平面直角坐标系和极坐标系对应两种不同的正交基，就代表了两种不同的描述方式，同样的点对应的向量不同。
 
**矩阵相关**
* 线性空间的另外一个重要特征就是承载变化。运动是空间的本质特性之一
* 例如：当线性空间中标准正交基确定之后，空间中的点就可以用向量表示，当这个点从一个位置移动到另外一个位置时，其向量发生变化。
* 点的变化对应着线性变换，而描述这种变化的数学语言就是矩阵。
* 思考下：可以自由定义任意空间，每种空间必须为之定义一定的运动（即变换），例如：线性空间 + 线性变换， 射影空间 + 射影变换等。
* 在线性空间中，变化有两种：点本身的变化，参考系的变化。即，坐标系不变，点变，则用矩阵乘以点向量。反过来，点向量值不变，必须要换个坐标系来观察。
* ${\bf{Ax = y}}$的解读：
    * 向量x经过矩阵A变换为向量y
    * 也可以理解为一个对象y在坐标系A下得到的结果为向量x，在标准坐标系的度量下得到的结果为向量y.因为：${\bf{Ax = Iy}}$
* 所以：矩阵不仅可以描述变化，也可以表示参考系本身。
* 矩阵和矩阵相乘的理解：就是对坐标系进行变化，让表示原始坐标系的矩阵可以表示变换的矩阵相乘，就变成了另外一个坐标系。
* 矩阵的描述：特征值和特征向量。${\bf{Ax}} = \lambda {\bf{x}}$, $\lambda$就是特征值，X就是特征向量。
* 矩阵特征值和特征向量的动态意义在于表示了变化的速度和方向。
* 求给定矩阵的特征值和特征向量的过程叫做特征值分解。能够进行分级的矩阵必须是n维方阵，将特征值分解算法推广到所有矩阵之上就是更加通用的奇异值分解。
* 几个要点：
    * 线性代数的本质在于将具体事物抽象为数学对象，并描述其静态和动态特性
    * 向量的实质是n维线性空间中的静止点
    * 线性变换描述了向量或者作为参考系的坐标系的变化，可以用矩阵表示
    * 矩阵的特征值和特征向量描述了变化的速度和方向

## 2. 概率论
概率论代表AI中的连接主义学派的技术基础
关注的焦点： 可能性

**频率学派**
* 频率学派：从事件发生的频率认识概率。
    * 古典概率模型： $P(A) = \dfrac{k}{n}$
    * A 为事件，其包含k个基本事件，所有基本事件的数目为n
* 上面是指当个随机事件，如果要刻画两个随机事件的关系，则需要引入“条件概率”
    * 假如事件A和事件B， 条件概率指事件A在事件B已经发生的条件下的概率用公式： $P(A|B) = \dfrac{P(AB)}{P(B)}$
    * 其中P(AB)叫做联合概率，表示A和B同时发生的概率。如果两个时间独立，则：P(AB) = P(A)P(B)。
    * 对于相互独立的事件，条件概率就是自身的概率：P(A|B) = P(A)
* 全概率公式为：$P(A) = \sum_{i = 1}^{N}P(A|B_i)\cdot P(B_i)$, $\sum_{i = 1}^{N} P(B_i) = 1$。
* 全概率公式的作用是将复杂时间的概率转化为在不同情况下发生的简单事件的概率和。解决问题思路：先做出一些假设，即得到Bi的概率，然后再在这些假设下讨论P(A|Bi)的条件概率，再来求事件A的概率。

**贝叶斯学派**
* 逆概率：在事件A的概率确定的情况下， 推断各种假设发生的可能性, 即P(Bi|A)。这套公式由贝叶斯提出，也就是贝叶斯公式，即：
$P(B_i|A)=\frac{P(A|B_i)\cdot P(B_i)}{\sum _{j=1}^{N} P(A|B_j)\cdot P(B_j)}$
* 由贝叶斯公式，可以进一步推出，贝叶斯定理：
$P(H|D) = \dfrac{P(D|H) \cdot P(H)}{P(D)}$, 其中P(H)为先验概率，即预先设定的假设成立的概率；P(D|H)被称为似然概率，即在假设成立的前提下观测到的结果的概率；P(H|D)称为后验概率，即在观测到结果的前提下假设成立的概率。
* 贝叶斯定理给出了一种全新的逻辑：根据观测结果寻找合适的假设，或者说根据观测数据寻找最佳的理论解释，关注的焦点为后验概率。
* 理念：概率描述的是随机事件的可信程度。

**频率学派和贝叶斯学派**
* 频率学派认为假设是客观存在且不会改变的，即存在固定的先验分布。在计算概率时，先确定概率分布的类型和参数，以此为基础进行概率推演。
* 贝叶斯学派：认为固定的先验分布式不存在的，参数本身也是随机的。也就是说，假设本身取决于观察结果，是不确定且可以修正的。数据的作用就是对假设做出不断修正，是观察者对概率的主观认识更加接近客观实际。
* 概率是机器学习的基础， 由于实际任务中可以使用的训练数据有限，因此需要对概率分布的参数进行估计，这是机器学习的核心任务。

**概率估计**
* 两种：最大似然估计法和最大后验概率法，分别代表频率学派和贝叶斯学派。
* 最大似然估计法：使训练数据出现的概率最大化，从而确定概率分布中的参数，估计出得概率分布最符合训练数据的分布。
* 最大后验概率法：根据训练数据和已知条件，使未知参数出现的的可能性最大化，并选取最可能的未知参数取值作为估计值。
* 在估计参数是，最大似然法只需要使用训练数据，最大后验概率法除了训练数据外，还要额外信息，就是贝叶斯公式中的先验概率。
* 在AI领域中，贝叶斯定理的各种方法和人类的认知更吻合，所以更加重要。

**随机变量**
* 离散随机变量、连续随机变量，每个变量的取值都有对应的概率。
* 离散变量的取值和概率之间一一对应就是离散随机变量的概率分布，也叫概率质量函数。
* 连续型随机变量的概率分布对应的就是概率密度函数。
* 特别说明的是： 概率密度函数体现的并非连续随机变量的真是概率，而是不同取值可能性之间的相对关系。其实，每个点的概率都是无穷小，取极限就是0.而概率密度函数的作用就是对这些无穷小量加以区分。例如：虽然$x \rightarrow \infty$时，1/x和2/x都是无穷小，但是后者永远是前者的2倍。所以，是相对意义。对概率密度函数进行积分，得到的才是连续随机变量的取值落在某个区间的概率。

**重要的分布**
* 重要的离散分布有两点分布、二项式分布、泊松分布。
* 重要的连续分布包括：均有分布、指数分布、正态分布。
* 两点分布： 适合随机试验的结果是二进制的情形，事件发生/不发生的概率分别是p/(1-p)。任何只有两个结果的随机试验都可以用两点分布来描述，例如，抛一次硬币的结果就是。
* 二项式分布：将满足参数为p的两点分布的随机试验独立重复n次，事件发生的次数即满足参数(n,p)的二项式分布。二项式分布的表达式可以写成：
$P(X = k) = C^n_k \cdot p ^ k \cdot (1 - p) ^ {(n - k)}, 0 \le k \le n$
* 泊松分布：放射性物质在规定事件内释放的粒子数所满足的分布，参数为$\lambda$的泊松分布表达式为：$P(X = k) = \lambda ^ k \cdot e ^ {-\lambda} / (k!)$。当二项式分布中的n很大且p时，其概率值可以由参数为$\lambda = np$的泊松分布的概率近似。
* 均匀分布：在区间(a,b)上满足均匀分布的连续型随机变量，其概率密度函数为：1/(b-a)，这个变量落在区间(a,b)内任意等长度的子区间的可能性相同。
* 指数分布：满足参数为$\Theta$指数分布的随机变量只能取正值，其概率密度函数为$e ^ {-x / \theta} / \theta, x > 0$，指数分布的一个重要特性是无记忆性，即P(X > s+t | X > s) = P(X > t)
* 正态分布：概率密度函数为：
$f(x) = \dfrac{1}{\sqrt{2\pi}\sigma} \cdot e ^ {-\frac{(x - \mu) ^ 2}{2\sigma ^ 2}}$, 当 $\mu = 0, \sigma = 1$时，上式称为标准正态分布。正态分布是最常见最重要的一种分布，自然界中很多现象近似服从正态分布。

**随机变量的数字特征**
* 数学期望、方差、协方差
* 数学期望和方差描述的是当个随机变量的数字特征，均值即随机变量的加权平均。方差表示的是随机变量取值与数字期望的偏离程度。方差小意味着随机变量的取值集中在数字期望附近，否则比较分散。
* 协方差度量两个随机变量之间的线性相关性。即Y变量可否表示为另外一个变量X为自变量的aX+b的形式。
* 根据协方差可以进一步求出相关系数，相关系数是一个绝对值不大于1的常数，等于1，意味着这个两个随机变量完全正相关，等于-1表示完全负相关，等于0表示不相关。
* 特别说明：无论是协方差还是相关系数，刻画的都是线性相关性，不能表达非线性相关性。


**核心知识点**
* 概率关注的是生活中的不确定性和可能性
* 频率学派认为先验分布是固定的，模型参数要靠最大似然法估计
* 贝叶斯学派认为先验分布是随机的，模型参数要靠后验概率最大化计算
* 正态分布是最重要的一种随机变量分布

**举例**
* 一个优等生和一个差生打架，老师肯定认为是差生的错，因为差生爱惹事，这就是最大似然估计；可如果老师知道优生和差生之间原本就有过节（先验信息），把这个因素考虑进来，就不会简单地认为是差生挑衅，这就是最大后验估计。


## 3. 数理统计
AI中统计的价值：有助于对机器学习的算法和数据挖掘的结果做出解释。
意义：根据观察或者实验得到的数据来研究随机现象，并对研究对象的客观规律做出合理的估计和判定。

**概率和统计**
* 概率：已知随机变量的分布，根据分布来分析随机变量的特征和规律
* 统计：针对未知分布的随机变量，研究方法是对随机变量进行独立重复的观察，根据观察结果对原始分布做出判定。
* 统计可以看成是逆向的概率
* 数理统计中，可用的数据集合叫做样本（sample），相应的，观察对象所有的可能取值被称为总体（population）。
* 数理统计的任务：根据样本推断总体的数字特征。通常，样本总是由对总体进行对此独立的重复观测而得到，保证了不同样本之间相互独立，并且都和总体具有相同的分布。
* 在统计推断中，应用的往往不是样本本身，而是被称为统计量的样本的函数。统计量本身就是个随机变量，是用来进行统计推断的工具。样本均值和样本方差是两个最重要的统计量：

**统计概念**
* 样本均值：${\bar X} = \dfrac{1}{n} \sum\limits_{i = 1}^{n} X_i$
* 样本方差：$S ^ 2 = \dfrac{1}{n - 1} \sum\limits_{i = 1}^{n} (X_i - {\bar X}) ^ 2$
* 统计推断的基本问题可以分为两类：参数估计(estimation theory)和假设检验(hypothesis test)

**参数估计**
* 通过随机抽取的样本类估计总体分布
* 分为：点估计和区间估计

**点估计**
* 点估计：在一致总体分布函数形式，但未知一个或者多个参数时，借助总体的一个样本来估计未知参数的取值就是参数的点估计。
* 核心：构造合适的统计量$\hat \theta$, 并且用这个统计量的观察值作为未知参数$\theta$的近似值。
* 分为：矩估计法(method of moments)和最大似然估计法
* 矩表示随机变量的分布特征， K阶矩定义为随机变量的k次方的均值，即$E(X^k)$, 思想在于用样本的k阶矩来估计总体的k阶矩。其理论依据在于样本矩的函数几乎处处收敛于总体矩的相应函数。
* 最大似然估计法源于频率学派看待概率的方法：既然抽样得到的是已有的样本值，可以认为取到这一组样本值得概率较大，因而在估计参数是就需要让已有样本值出现的可能性最大。
* 似然函数被定义为样本观测值出现的概率，让似然函数值最大化，就是微积分中求解函数最大值的问题。由于不同的样本值之间独立，因而似然函数可以写成若干概率质量函数/概率密度函数相乘的形式，并进一步转化为对数方程求解。
* 矩估计和最大似然估计代表两种推断总体参数的思路。估计的评价有三个基本标准：
    * 无偏性：估计量的数学期望值等于位置参数的真实值
    * 有效性：无偏估计量的方差尽可能小
    * 一致性：当样本容量接近无穷是，估计量依照概率收敛于未知参数的真实值。

**区间估计**
* 在估计未知参数$\theta$值，还需要估计出一个区间，并且确定这个区间包含此真实值的可信程度。在数理统计中，这个区间叫做置信区间
* 这种估计方式称为区间估计
* 方法：对总体反复抽样多次，每次得到容量相同的样本，则根据每一组样本值可以确定出一个置信区间$(\underline\theta,\overline\theta)$,其上界和下界是样本的两个统计量，分别代表置信上界和下界。
* 区间估计：在点估计基础上提供了取值范围和误差界限，分别对应置信区间和置信水平。


**假设检验**
* 参数估计的对象是总体的某个参数，假设检验的对象则是关于总体的某个论断，即关于总体的假设。
* 假设检验中的假设包含原假设$H_0$和备择假设$H_1$, 检验的过程就是根据样本在它们之间选择一个接受。
* 理想情况是$H_0$、$H_1$为真并且接受$H_0$。但是往往存在问题，错误的决策终归出现，其形式分为2种：（1）对应假设$H_0$为真，但被拒绝了。（2）对应$H_0$为假，但被接受了。
* 假设检验的思想：全称命题只能够被证伪不能被证实的基础上。要证明$H_0$为真，证明$H_1$为假好了。
* 证伪通过小概率事件来证明，小于1%的概率发生的事件叫做小概率事件。
* 从统计学习的角度看：监督学习算法的任务就是在假设空间中搜索能够针对特定问题作出良好预测的假设。学习期通过对测试数据集的学习得到具有普适性的模型，这个模型适用于不属于测试集的新样本的能力成为泛化能力。
* 假设检验的作用在于根据学习器在测试集上的性能推断其泛化能力的强弱，并确定所得结论的精确程度，可以进一步推广为比较不同学习器的性能。
* 度量学习器的常用指标是错误率。
* 泛化误差的构成：
  * 偏差：刻画预测值和真实值之间的偏差程度，刻画的是模型的欠拟合。
  * 方差：表示数据的扰动对预测性能的影响，刻画的是模型的过拟合特性。
  * 噪声：表示在当前学习任务上能够达到的最小泛化误差，刻画的是任务本身的难度。
  * 偏差和方差难以同时优化，反映出欠拟合和过拟合难以调和

**知识要点**

* 数理统计的任务是根据可观察的样本反过来推断总体的性质
* 推断的工具是统计量，统计量是样本的函数，是个随机变量。
* 参数估计通过随机抽取的样本来估计总体分布的未知参数，包括点估计和区间估计
* 假设检验通过随机抽取的样本来接受或拒绝关于总体的某个判断，常用语估计机器学习模型的泛化错误率。

## 4. 最优化方法

本质上来说，AI的目标就是最优化：**在复杂环境与多体交互中做出的最优决策**

* 最优化理论研究的问题是判定给定目标函数的最大值或者最小值是否存在，并找到令目标函数取得最大值、最小值的数值。
* 目标函数（objective function）又称为评价函数，就是$f(x)$
* 实际的最优化算法的目标是找到全局的最小值，也可能找到局部极小值。理想状态下找到全局最小值，实际上，受到视野限制，目前使用的最优化算法不具备这样的上帝视角，它们是站在山脚下去寻找山峰，经常只能找到局部极小值。
* 分类：无约束优化和约束优化
* 线性规划：一种典型的约束优化，通常通过拉格朗日乘子的引入可以把含有n个变量，k个约束条件的问题转化为(n+k)个变量的无约束优化问题：$L(x, y, \lambda) = f(x, y) + \lambda \varphi(x, y)$，其中$f(x,y)$ 为目标函数， $\varphi(x,y)$为等式约束条件，$\lambda$ 是拉格朗日乘数。
* 无约束优化问题，最常用的方法是：梯度下降法。梯度的方向就是目标函数倒数的反方向。当函数的输入为向量时，目标函数的图像就变成了高维空间上的曲面，其梯度就是垂直曲面等高线并指向高度增加方向的向量。优化问题变成了“多云函数沿负梯度方向下降最快”，就是梯度下降法的理论依据。
* 梯度下降法，“步长”很重要，就是每次更新$f(x)$时x的变化值。步长选择整体规律是逐步变小的。
* 两种常用方法：
  * 批处理模式：计算每个样本上目标函数的梯度，将不同样本的梯度求和，结果作为本次更新中目标函数的梯度。在批处理模式中，每次更新都要便利训练所有样本，运算量很大。
  * 随机梯度下降法：每次更新只用一个样本，下次更新使用另外一个样本，有趣的是，当训练集规模较大时，随机梯度下降法性能更佳。
* 梯度下降法只用到了函数的一阶导数，没有使用二阶导数。一阶导数描述的是函数随输入的变化，二阶导数提供了目标函数曲率信息，当曲率为正，则目标函数比梯度下降法的预期要慢，如果曲率为负，则预期下降更快。
* 所以，二阶导数包含的全局信息能够为梯度下降的方向提供指导，进而可以获得更优的收敛性。

**牛顿法**
* 就是将二阶导数引入优化过程
* 目标函数先被泰勒展开，写出二阶近似形式
* 对二阶近似后的目标函数求导，令其等于0，得到的向量表示就是下降最快的方向。
* 牛顿法收敛更快
* 不管是什么，基本思路：先确定方向，再确定步长，因而统称为“线性搜索法”

**置信域方法**
* 先确定步长，以步长为参数划定一个区域，再在这个区域寻找最快下降的方向
* 设定一个置信域$\delta$, 并以它为中心，以$s$为半径的封闭球形区域为置信域，在置信域中寻找目标函数的二次近似模型的最优点，最优点和当前点之间的距离就是计算出来的备选位移。
* 在备选位移上，如果目标函数的二次近似产生了充分的下降，就将当前点移动到计算出的最优点，继续迭代下去，并可以适当增加$s$；如果目标函数的近似下降不够理想，则说明步子跨太大，需要缩小它并计算新的备选位移，直到满足终止条件。

**启发式算法**
* heuristics, 来源于20世纪50年代的仿生学。
* 核心思想：大自然的“优胜劣汰”的生存法则，并在算法的实现中添加了选择和突变等经验因素。
* 例如，遗传算法、模拟退火、蚁群算法等。
* 神经网络实际上是一类启发式算法，模拟大脑中神经元竞争和协作机制。

**知识要点**
* 最优化问题是在无约束或者约束条件下，求解给定目标函数的最小值
* 线性搜索中，确定寻找最小值时的搜索方向需要使用目标函数的一阶导数和二阶导数。
* 置信域算法的思想是先确定搜索步长，再确定搜索方向。
* 以人工神经网路为代表的启发式算法是另外一种重要的优化方法。
  
## 5. 信息论
起源：1948年，香农发表了著名论文《通信的数学理论》，给出了对信息概念的定量分析方法。
香农的说法：
> 通信的基本问题是在一点精确或近似的复现另外一点所选取的消息。消息通常有意义，本身指向或者关联着物理上或者概念上的特定实体。消息的语义含义与工程问题无关，重要的问题是一条消息来自于一个所有可能的消息集合。

**基本概念**
* 所有类型的信息都可以抽象为逻辑符号
* 信息论使用“信息熵”的概念，解决的问题
  * 信息量和通信中传递信息的数量做出了解释
  * 在世界的不确定性和信息的可测量性建立了联系
* shannon利用了“熵”对信息进行量化。熵的本质是，描述一个系统内在的混乱程度，越混乱，熵越高。
* 如果事件$A$发生的概率为$p(A)$，则这个事件的自信息量的定义为：
  $h(A)=-\log_2p(A)$
* 根据单个事件的自信息量可以计算包含多个符号的信源的信息熵。信源$X$的信源熵定义：  $H(X) = - \sum\limits_{i=1}^n p(a_i)\log_2p(a_i)$, 信源熵描述了每个信源发送一个符号所提供的平均信息量。当信源中的每个符号的取值概率相等，则信源熵取到的最大值为$-\log_2 n$，意味着信源的随机程度最高。
* 通过概率中的条件概率，可以推演出条件熵的概念：$H(Y|X)$,表示已知随机变量$X$的条件下，$Y$的不确定性，也就是在给出$X$时，根据$Y$的条件概率计算出熵，在对$X$求数学期望
    >$H(Y|X)=\sum\limits_{i=1}^n p(x_i)H(Y|X=x_i)$   
    $= -\sum_{i = 1}^ n p(x_i) \sum_{j = 1}^ m p(y_j|x_i) \log_2 p(y_j|x_i)$  
    $= - \sum_{i = 1}^ n \sum_{j = 1}^ m p(x_i, y_j) \log_2 p(y_j|x_i)$  
* 条件熵的意义: 先按照变量$X$的取值对$Y$进行了一次分类， 对每个分出来的类别计算其单独的信息熵，再将每个类的信息熵按照$X$的分布计算其数学期望值
* 互信息： $I(X;Y) = H(Y) - H(Y|X)$, 即$X$提供的关于$Y$的不确定性的消除，也可以看出是$X$给$Y$带来的信息增益。互信息在通信领域经常用，信息增益在机器学习领域经常用。本质上是一样的。

**机器学习中的信息论应用**
* 在机器学习中，信息增益常用于分类特征选择。例如：给定数据集$Y$, $H(Y)$表示未给定任何特征是，对训练集进行分类的不确定性；$H(Y|X)$表示使用了特征$X$对训练集$Y$进行分类的不确定性。信息增益表示的就是特征$X$带来的对训练集$Y$分类不确定性的减少程度，也就是特征$X$对训练集$Y$的区分度。
* 显然，信息增益更大的特征具有更强的分类能力。但是，信息增益很大程度依赖于数据集的信息熵$H(Y)$, 因而不具有绝对意义。所以，提出了信息增益比的概念，即：$g(X,Y) = I(X;Y)/H(Y)$
* KL散度：描述两个概率分布$P$ 和 $Q$之间的差异的一种方法，定义为：$D_{KL}(P||Q) = \sum_{i = 1}^n p(x_i) \log_2 \frac{p(x_i)}{q(x_i)}$
* KL散度是对额外信息量的衡量。给定一个信源，如果其符号概率分布为$P(X)$, 就可以设计一种最有编码，使得表示信源所需要的平均比特数最少（等于该信源的信息熵）。
* KL散度特性： 非负性，非对称性，即，$D_KL(P||Q) \ne D_KL(Q||P)$, 因此，KL散度不满足数学意义上对距离的定义。
* 最大熵原理：对于一个未知的概率分布，最坏的情况是它以等可能性的取到每个可能的取值。因而可以得到最不确定的结果，预测的风险最小。“不把所有鸡蛋放在同一个篮子里”，可以视为最大熵原理的一个具体应用。
* 将最大熵原理应用到分类问题熵，就可以得到最大熵模型。
* 在分类问题中，首先要确定若干个特征函数作为分类的一句。每一个特征函数对应了一个约束条件。分类的任务就是在这些约束条件下，确定一个最好的分类模型。由于除了这些约束条件外，没有任何关于分类的先验知识，因此需要利用最大熵原理，求解出不确定性最大的条件分布，即让下面函数取最大值：
$H(p) = -\sum\limits_{x, y} \tilde p(x) p(y|x) \log_2 p(y|x)$，式中，$p(y|x)$就是分类问题要确定的目标条件分布。上面其实就是一个约束优化问题，由特征值函数确定的约束条件可以通过拉格朗日乘子的引入去除影响，转化为一个无约束优化问题。从数学上可以证明，这个模型的解存在且唯一。

**知识要点**
* 信息论处理的是客观世界中的不确定性
* 条件熵和信息增益是分类问题中的重要参数
* KL散度用于描述两个不同概率分布之间的差异
* 最大熵原理是分类问题中的常用准则
* 信息论建立在概率的基础熵，但是形式并不唯一，除了香农熵之外还有其他关于熵的定义。

## 6. 形式逻辑
理想的AI：具备学习、推理和归纳的能力，形式逻辑是基础。早期研究者认为，人类认知和思维的基本单元是符号，而认知过程就是对符号的逻辑运算。这样，人类抽象的逻辑思维就可以通过计算机中逻辑门的运算模拟，进而实现机械化的人类认知。

**基本概念**
* AI的基础首先是知识的表示。形式逻辑是实现知识表示的一种普遍方法。
* 例如：著名的三段论
* AI中应用的主要是一阶谓词逻辑。谓词逻辑的一个特例是命题逻辑。在命题逻辑中，命题是逻辑处理的基本单位，只能对其真伪做判断。
* 谓词逻辑中，命题被拆分为个体词、谓词和量词，三者的意义如下：
  * 个体词： 可以独立存在的具体或者抽象的描述对象，类似生活中的“对象”、“物体”等名词。
  * 谓词：描述个体词的属性和相互关系，例如“是....的父亲”
  * 量词：描述个体词的数量关系，包括全称量词$\forall$和存在量词$\exists$。
* 以上三种元素可以共同构成命题。不同命题之间可以通过逻辑连接词建立联系，由简单命题形成符合命题。逻辑连接词包括五种：
  * 否定($\neg$)
  * 合取($\wedge$)
  * 析取($\vee$)
  * 蕴涵($\to$)
  * 等价($\leftrightarrow$)
* 在谓词逻辑中出现的可以是常量、变量、函数符号等。
* 谓词逻辑可以表示： 事物的特性， 事物间具有确定因果关系的规则性知识。
* 事实性知识通常采用析取与合取符号连接起来的谓词公式表示；规则性知识则常使用由蕴含符号连接起来的谓词公式表示。
* 使用谓词逻辑进行知识表示的步骤：
  * 定义谓词和个体
  * 根据要表达的事物和概念，为每个谓词中的变量赋予特定的值
  * 根据要表达的知识的语义，用适当的逻辑连接词将每个谓词连接起来。
* 人类智能和人工智能的主要区别体现在推理能力上。
* AI实现自动推理的基础是产生式系统：
  * 以产生式的规则描述符号串来替代运算
  * 把推理和行为的过程用产生式规则表示，被早期大多数专家系统使用。
  * 一般包含：规则库，事实库，推理机。
    * 规则库：存储着产生式形式表示的规则集合。
    * 事实库：存储的是输入事实、中间结果和最终结果，当规则库中的某条产生式的前提可与事实库中的某些已知事实匹配，该产生式被激活，其结论就可以作为已知事实存储在事实库。
    * 推理机：用于控制和协调规则库和事实库运行的程序，包括推理方式和控制策略。
  * 推理方式：正向、反向和双向推理三种。

**自动推理和人工智能**
* 自动推理在数学证明上能力不错，可在解决日常生活中问题时表现一般，主要原因时常识的缺失。
* 哥德尔不完备性定理：
  * 第1不完备性定理：在任何包含初等数论的形式系统中，都必定存在一个不可判定命题。主要是，对“自指”的无能为力，例如：“本数学命题不可以被证明”，无法证实也不能证伪。
* 在哥德尔不完备性定理的阴影下，基于图灵可计算概念的“认知可计算主义”表现了极大的局限性。
* 所以，现在，依靠人工神经网络崛起的“连接主义”学派是主流，而以形式逻辑为依据的符号主义学派走向没落。
  
**主要知识点**
* 如果将认知定义为对符号的逻辑运算，AI的基础就是形式逻辑
* 谓词逻辑是知识表示的主要方法
* 基于谓词逻辑系统可以实现具有自动推理能力的AI
* 不完备性定理向“认知的本质是计算”提出挑战
* 符号主义思路更接近人类认知思路，对形式逻辑的处理能否成为依赖小数据学习的AI的核心技术？


