# AI相关数学基础
## 参考内容
[1] 极客时间：《人工智能基础》, 王天一
## 1. 线性代数
**基本概念**
* 集合(set), 由具有共性的对象构成的全体。
* 单个数叫做标量（scalar），一个标量可以是实数或者**复数**，注意，在数学中，复数是最常用的“域”，即满足象代数中“域”定义的集合，实数域是复数域的特例而已。
* 如果多个标量按照一定顺序组成一个序列，就是向量， vector。
* 向量的每个元素又是一个向量的话，就是矩阵，matrix
* 如果将矩阵中的每个标量元素换成向量的话，就得到张量， tensor
* 线性代数式用“虚拟数字”表示真实物理世界的工具。想想“万物皆数”的理念。
* 计算机的量：标量占据零维数组；向量为一维数组，例如语音信号；矩阵是二维数组，例如灰度图像；张量是三维或者更高维数组，例如RGB图像和视频。
* 范数（norm）：单个向量大小的度量，作用是将向量映射到一个非负的数值。通常的$L ^ p$范数的定义为：${\left| {\bf{x}} \right|_p} = {\left( {\sum\limits_i {{{\left| {{x_i}} \right|}^p}} } \right)^{\frac{1}{p}}}$, 所以$L^1$范数就是向量所有元素绝对值的和；$L^2$范数就是向量的长度； $L ^ {\infty}$就是向量中最大元素的取值。范数计算的是单个向量的尺度。
* 接下来考虑两个向量的关系，一般用内积(inner product)。计算表达式为：$\left\langle {{\bf{x,y}}} \right\rangle  = \sum\limits_i {{x_i} \cdot {y_i}}$, 内积表示的是两个向量的夹角。如果<x,y>=0, 则表示夹角为90度，即$\cos \Theta =0$, 即两个向量垂直。在高维空间中，这种关系被称为正交。 如果两个向量正交，则线性无关，相互独立。
* 线性空间：一个包含相同维数的向量集合，并且定义了加法和数乘运算，这样的集合称为线性空间。例如：平时的实数域、复数域、二维、三维空间，都是线性空间。
* 定义了内积运算的线性空间称为内积空间。
* 在线性空间中，任意向量代表的都是n维空间的一个点，反过来，空间中的任一点都可以唯一地用一个向量表示，两者等效。
* 空间中的点和向量的映射中，参考的选择很关键，就是说不通的坐标系（代表不通的基）对应的向量值不同。
* 在内积空间中，一组两两正交的向量构成空间的正交基。如果这些正交基的L-2范数为1，则为标准正交基。定义了正交基，就确定了向量和点之间的对应关系。
* 内积空间的正交基不唯一，例如：二维空间，平面直角坐标系和极坐标系对应两种不同的正交基，就代表了两种不同的描述方式，同样的点对应的向量不同。
 
**矩阵相关**
* 线性空间的另外一个重要特征就是承载变化。运动是空间的本质特性之一
* 例如：当线性空间中标准正交基确定之后，空间中的点就可以用向量表示，当这个点从一个位置移动到另外一个位置时，其向量发生变化。
* 点的变化对应着线性变换，而描述这种变化的数学语言就是矩阵。
* 思考下：可以自由定义任意空间，每种空间必须为之定义一定的运动（即变换），例如：线性空间 + 线性变换， 射影空间 + 射影变换等。
* 在线性空间中，变化有两种：点本身的变化，参考系的变化。即，坐标系不变，点变，则用矩阵乘以点向量。反过来，点向量值不变，必须要换个坐标系来观察。
* ${\bf{Ax = y}}$的解读：
    * 向量x经过矩阵A变换为向量y
    * 也可以理解为一个对象y在坐标系A下得到的结果为向量x，在标准坐标系的度量下得到的结果为向量y.因为：${\bf{Ax = Iy}}$
* 所以：矩阵不仅可以描述变化，也可以表示参考系本身。
* 矩阵和矩阵相乘的理解：就是对坐标系进行变化，让表示原始坐标系的矩阵可以表示变换的矩阵相乘，就变成了另外一个坐标系。
* 矩阵的描述：特征值和特征向量。${\bf{Ax}} = \lambda {\bf{x}}$, $\lambda$就是特征值，X就是特征向量。
* 矩阵特征值和特征向量的动态意义在于表示了变化的速度和方向。
* 求给定矩阵的特征值和特征向量的过程叫做特征值分解。能够进行分级的矩阵必须是n维方阵，将特征值分解算法推广到所有矩阵之上就是更加通用的奇异值分解。
* 几个要点：
    * 线性代数的本质在于将具体事物抽象为数学对象，并描述其静态和动态特性
    * 向量的实质是n维线性空间中的静止点
    * 线性变换描述了向量或者作为参考系的坐标系的变化，可以用矩阵表示
    * 矩阵的特征值和特征向量描述了变化的速度和方向

## 2. 概率论
概率论代表AI中的连接主义学派的技术基础
关注的焦点： 可能性

**频率学派**
* 频率学派：从事件发生的频率认识概率。
    * 古典概率模型： $P(A) = \dfrac{k}{n}$
    * A 为事件，其包含k个基本事件，所有基本事件的数目为n
* 上面是指当个随机事件，如果要刻画两个随机事件的关系，则需要引入“条件概率”
    * 假如事件A和事件B， 条件概率指事件A在事件B已经发生的条件下的概率用公式： $P(A|B) = \dfrac{P(AB)}{P(B)}$
    * 其中P(AB)叫做联合概率，表示A和B同时发生的概率。如果两个时间独立，则：P(AB) = P(A)P(B)。
    * 对于相互独立的事件，条件概率就是自身的概率：P(A|B) = P(A)
* 全概率公式为：$P(A) = \sum_{i = 1}^{N}P(A|B_i)\cdot P(B_i)$, $\sum_{i = 1}^{N} P(B_i) = 1$。
* 全概率公式的作用是将复杂时间的概率转化为在不同情况下发生的简单事件的概率和。解决问题思路：先做出一些假设，即得到Bi的概率，然后再在这些假设下讨论P(A|Bi)的条件概率，再来求事件A的概率。

**贝叶斯学派**
* 逆概率：在事件A的概率确定的情况下， 推断各种假设发生的可能性, 即P(Bi|A)。这套公式由贝叶斯提出，也就是贝叶斯公式，即：
$P(B_i|A)=\frac{P(A|B_i)\cdot P(B_i)}{\sum _{j=1}^{N} P(A|B_j)\cdot P(B_j)}$
* 由贝叶斯公式，可以进一步推出，贝叶斯定理：
$P(H|D) = \dfrac{P(D|H) \cdot P(H)}{P(D)}$, 其中P(H)为先验概率，即预先设定的假设成立的概率；P(D|H)被称为似然概率，即在假设成立的前提下观测到的结果的概率；P(H|D)称为后验概率，即在观测到结果的前提下假设成立的概率。
* 贝叶斯定理给出了一种全新的逻辑：根据观测结果寻找合适的假设，或者说根据观测数据寻找最佳的理论解释，关注的焦点为后验概率。
* 理念：概率描述的是随机事件的可信程度。

**频率学派和贝叶斯学派**
* 频率学派认为假设是客观存在且不会改变的，即存在固定的先验分布。在计算概率时，先确定概率分布的类型和参数，以此为基础进行概率推演。
* 贝叶斯学派：认为固定的先验分布式不存在的，参数本身也是随机的。也就是说，假设本身取决于观察结果，是不确定且可以修正的。数据的作用就是对假设做出不断修正，是观察者对概率的主观认识更加接近客观实际。
* 概率是机器学习的基础， 由于实际任务中可以使用的训练数据有限，因此需要对概率分布的参数进行估计，这是机器学习的核心任务。

**概率估计**
* 两种：最大似然估计法和最大后验概率法，分别代表频率学派和贝叶斯学派。
* 最大似然估计法：使训练数据出现的概率最大化，从而确定概率分布中的参数，估计出得概率分布最符合训练数据的分布。
* 最大后验概率法：根据训练数据和已知条件，使未知参数出现的的可能性最大化，并选取最可能的未知参数取值作为估计值。
* 在估计参数是，最大似然法只需要使用训练数据，最大后验概率法除了训练数据外，还要额外信息，就是贝叶斯公式中的先验概率。
* 在AI领域中，贝叶斯定理的各种方法和人类的认知更吻合，所以更加重要。

**随机变量**
* 离散随机变量、连续随机变量，每个变量的取值都有对应的概率。
* 离散变量的取值和概率之间一一对应就是离散随机变量的概率分布，也叫概率质量函数。
* 连续型随机变量的概率分布对应的就是概率密度函数。
* 特别说明的是： 概率密度函数体现的并非连续随机变量的真是概率，而是不同取值可能性之间的相对关系。其实，每个点的概率都是无穷小，取极限就是0.而概率密度函数的作用就是对这些无穷小量加以区分。例如：虽然$x \rightarrow \infty$时，1/x和2/x都是无穷小，但是后者永远是前者的2倍。所以，是相对意义。对概率密度函数进行积分，得到的才是连续随机变量的取值落在某个区间的概率。

**重要的分布**
* 重要的离散分布有两点分布、二项式分布、泊松分布。
* 重要的连续分布包括：均有分布、指数分布、正态分布。
* 两点分布： 适合随机试验的结果是二进制的情形，事件发生/不发生的概率分别是p/(1-p)。任何只有两个结果的随机试验都可以用两点分布来描述，例如，抛一次硬币的结果就是。
* 二项式分布：将满足参数为p的两点分布的随机试验独立重复n次，事件发生的次数即满足参数(n,p)的二项式分布。二项式分布的表达式可以写成：
$P(X = k) = C^n_k \cdot p ^ k \cdot (1 - p) ^ {(n - k)}, 0 \le k \le n$
* 泊松分布：放射性物质在规定事件内释放的粒子数所满足的分布，参数为$\lambda$的泊松分布表达式为：$P(X = k) = \lambda ^ k \cdot e ^ {-\lambda} / (k!)$。当二项式分布中的n很大且p时，其概率值可以由参数为$\lambda = np$的泊松分布的概率近似。
* 均匀分布：在区间(a,b)上满足均匀分布的连续型随机变量，其概率密度函数为：1/(b-a)，这个变量落在区间(a,b)内任意等长度的子区间的可能性相同。
* 指数分布：满足参数为$\Theta$指数分布的随机变量只能取正值，其概率密度函数为$e ^ {-x / \theta} / \theta, x > 0$，指数分布的一个重要特性是无记忆性，即P(X > s+t | X > s) = P(X > t)
* 正态分布：概率密度函数为：
$f(x) = \dfrac{1}{\sqrt{2\pi}\sigma} \cdot e ^ {-\frac{(x - \mu) ^ 2}{2\sigma ^ 2}}$, 当 $\mu = 0, \sigma = 1$时，上式称为标准正态分布。正态分布是最常见最重要的一种分布，自然界中很多现象近似服从正态分布。
 
**随机变量的数字特征**
* 数学期望、方差、协方差
* 数学期望和方差描述的是当个随机变量的数字特征，均值即随机变量的加权平均。方差表示的是随机变量取值与数字期望的偏离程度。方差小意味着随机变量的取值集中在数字期望附近，否则比较分散。
* 协方差度量两个随机变量之间的线性相关性。即Y变量可否表示为另外一个变量X为自变量的aX+b的形式。
* 根据协方差可以进一步求出相关系数，相关系数是一个绝对值不大于1的常数，等于1，意味着这个两个随机变量完全正相关，等于-1表示完全负相关，等于0表示不相关。
* 特别说明：无论是协方差还是相关系数，刻画的都是线性相关性，不能表达非线性相关性。


**核心知识点**
* 概率关注的是生活中的不确定性和可能性
* 频率学派认为先验分布是固定的，模型参数要靠最大似然法估计
* 贝叶斯学派认为先验分布是随机的，模型参数要靠后验概率最大化计算
* 正态分布是最重要的一种随机变量分布

**举例**
* 一个优等生和一个差生打架，老师肯定认为是差生的错，因为差生爱惹事，这就是最大似然估计；可如果老师知道优生和差生之间原本就有过节（先验信息），把这个因素考虑进来，就不会简单地认为是差生挑衅，这就是最大后验估计。


## 3. 数理统计
AI中统计的价值：有助于对机器学习的算法和数据挖掘的结果做出解释。
意义：根据观察或者实验得到的数据来研究随机现象，并对研究对象的客观规律做出合理的估计和判定。

**概率和统计**
* 概率：已知随机变量的分布，根据分布来分析随机变量的特征和规律
* 统计：针对未知分布的随机变量，研究方法是对随机变量进行独立重复的观察，根据观察结果对原始分布做出判定。
* 统计可以看成是逆向的概率
* 数理统计中，可用的数据集合叫做样本（sample），相应的，观察对象所有的可能取值被称为总体（population）。
* 数理统计的任务：根据样本推断总体的数字特征。通常，样本总是由对总体进行对此独立的重复观测而得到，保证了不同样本之间相互独立，并且都和总体具有相同的分布。
* 在统计推断中，应用的往往不是样本本身，而是被称为统计量的样本的函数。统计量本身就是个随机变量，是用来进行统计推断的工具。样本均值和样本方差是两个最重要的统计量：

**统计概念**
* 样本均值：${\bar X} = \dfrac{1}{n} \sum\limits_{i = 1}^{n} X_i$
* 样本方差：$S ^ 2 = \dfrac{1}{n - 1} \sum\limits_{i = 1}^{n} (X_i - {\bar X}) ^ 2$
* 统计推断的基本问题可以分为两类：参数估计(estimation theory)和假设检验(hypothesis test)

**参数估计**
* 通过随机抽取的样本类估计总体分布
* 分为：点估计和区间估计

**点估计**
* 点估计：在一致总体分布函数形式，但未知一个或者多个参数时，借助总体的一个样本来估计未知参数的取值就是参数的点估计。
* 核心：构造合适的统计量$\hat \theta$, 并且用这个统计量的观察值作为未知参数$\theta$的近似值。
* 分为：矩估计法(method of moments)和最大似然估计法
* 矩表示随机变量的分布特征， K阶矩定义为随机变量的k次方的均值，即$E(X^k)$, 思想在于用样本的k阶矩来估计总体的k阶矩。其理论依据在于样本矩的函数几乎处处收敛于总体矩的相应函数。
* 最大似然估计法源于频率学派看待概率的方法：既然抽样得到的是已有的样本值，可以认为取到这一组样本值得概率较大，因而在估计参数是就需要让已有样本值出现的可能性最大。
* 似然函数被定义为样本观测值出现的概率，让似然函数值最大化，就是微积分中求解函数最大值的问题。由于不同的样本值之间独立，因而似然函数可以写成若干概率质量函数/概率密度函数相乘的形式，并进一步转化为对数方程求解。
* 矩估计和最大似然估计代表两种推断总体参数的思路。估计的评价有三个基本标准：
    * 无偏性：估计量的数学期望值等于位置参数的真实值
    * 有效性：无偏估计量的方差尽可能小
    * 一致性：当样本容量接近无穷是，估计量依照概率收敛于未知参数的真实值。

**区间估计**
* 在估计未知参数$\theta$值，还需要估计出一个区间，并且确定这个区间包含此真实值的可信程度。在数理统计中，这个区间叫做置信区间
* 这种估计方式称为区间估计
* 方法：对总体反复抽样多次，每次得到容量相同的样本，则根据每一组样本值可以确定出一个置信区间$(\underline\theta,\overline\theta)$,其上界和下界是样本的两个统计量，分别代表置信上界和下界。
* 区间估计：在点估计基础上提供了取值范围和误差界限，分别对应置信区间和置信水平。


**假设检验**
* 参数估计的对象是总体的某个参数，假设检验的对象则是关于总体的某个论断，即关于总体的假设。
* 假设检验中的假设包含原假设$H_0$和备择假设$H_1$, 检验的过程就是根据样本在它们之间选择一个接受。
* 理想情况是$H_0$、$H_1$为真并且接受$H_0$。但是往往存在问题，错误的决策终归出现，其形式分为2种：（1）对应假设$H_0$为真，但被拒绝了。（2）对应$H_0$为假，但被接受了。
* 假设检验的思想：全称命题只能够被证伪不能被证实的基础上。要证明$H_0$为真，证明$H_1$为假好了。
* 证伪通过小概率事件来证明，小于1%的概率发生的事件叫做小概率事件。
* 从统计学习的角度看：监督学习算法的任务就是在假设空间中搜索能够针对特定问题作出良好预测的假设。学习期通过对测试数据集的学习得到具有普适性的模型，这个模型适用于不属于测试集的新样本的能力成为泛化能力。
* 假设检验的作用在于根据学习器在测试集上的性能推断其泛化能力的强弱，并确定所得结论的精确程度，可以进一步推广为比较不同学习器的性能。
* 度量学习器的常用指标是错误率。
* 泛化误差的构成：
    * 偏差：刻画预测值和真实值之间的偏差程度，刻画的是模型的欠拟合。
    * 方差：表示数据的扰动对预测性能的影响，刻画的是模型的过拟合特性。
    * 噪声：表示在当前学习任务上能够达到的最小泛化误差，刻画的是任务本身的难度。
    * 偏差和方差难以同时优化，反映出欠拟合和过拟合难以调和

**知识要点**
* 数理统计的任务是根据可观察的样本反过来推断总体的性质
* 推断的工具是统计量，统计量是样本的函数，是个随机变量。
* 参数估计通过随机抽取的样本来估计总体分布的未知参数，包括点估计和区间估计
* 假设检验通过随机抽取的样本来接受或拒绝关于总体的某个判断，常用语估计机器学习模型的泛化错误率。





