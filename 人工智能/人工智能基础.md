# AI基础
## 参考内容
[1] 极客时间：《人工智能基础》, 王天一

## 1. 机器学习概论
简单来说，**机器学习就是从大量现象中提取反复出现的规律和模式**。
ML是计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的学科。 

**基本概念**
* 属性：对对象某些性质的描述
* 属性值：属性的取值
* 数据：不同属性值有序排列得到的向量，也叫做**实例**
* 根据线性代数知识，数据的不同属性可以视为相互独立，所以每个属性代表一个不同的维度，这些维度共同张成了**特征空间**
* 每个实例就是特征空间中的一个点，即**特征向量**， 这和线性代数中的特征向量和特征值概念不同。
* 误差：ML的实际预测输出与样本真实输出之间的差异，在分类中，常用的误差函数是**错误率**，误差可以进一步分为训练误差和测试误差。
* 训练误差：学习器在训练数据集上的误差，又称为经验误差
* 测试误差：学习器在新样本上的误差，又称为泛化误差。
* 过拟合：训练误差小，泛化误差大。
* 欠拟合：训练数据的基本性质都没有学到。
* 欠拟合可以通过改进学习器的算法克服，过拟合无法避免。
* 整体而言，**测试误差与模型复杂度之间呈现抛物线关系**，当模型复杂度较低时，测试误差较高；随着模型复杂度的增加，测试误差将逐渐下降并达到最小值；之后，当模型复杂度继续上升时，测试误差随之增加，对应着过拟合的发生。
* 在模型选择中，一种广泛使用的方法时交叉验证
* 除了算法本身，参数的取值也是影响模型性能的重要因素。因此，调参，也就是对算法参数的设定，时ML中重要的工程问题，这点在神经网络与深度学习中非常重要。
  * 举例：假如一个ANN（人工神经网络）包含1000个参数，每个参数有10中取值的可能，对于每一组训练/测试集就有$1000^{10}$个模型需要考察。因此，在调参过程中，一个主要的问题就是性能和效率的折中。

**ML预测问题的分类**
* **分类问题**：输出变量为有限个离散变量，当个数为2，即为简单的二分类问题
* **回归问题**：输入变量和输出变量均为连续变量
* **标注问题**：输入变量和输出变量均为变量序列

**ML学习的分类**
* **监督学习**：基于已知类别的训练数据进行学习
* **无监督学习**：基于未知类别的训练数据进行学习
* **半监督学习**：同时使用已知类比和未知类别的训练数据进行学习。
* 一般而言，效果较好的学习算法执行的都是监督学习的学习任务，号称自学成才的AlphaGo Zero, 其训练过程也要受到围棋规则的限制，也脱不开监督学习的范畴。

**监督学习**
* 假定训练数据满足独立同分布条件，并根据训练数据学习出一个由输入到输出的映射模型。
* 反映这种关系的模型可能有无数种，所有模型共同组成了假设空间。
* 监督学习的任务就是在假设空间中根据特定的误差准则寻找最优的模型
* 根据学习方法不同，可以分为：
  * 生成方法类别：根据输入数据和输出数据之间的联合概率确定条件概率$P(Y|X)$,这种方法表示了输入$X$和输出$Y$之间的生成关系。
  * 判别方法类别：直接学习条件概率分布$P(Y|X)$或者决策函数$f(X)$, 这种方法表示了根据输入$X$得出输出$Y$的预测方法。
  * 生成方法具有更快的收敛速度和更广的应用范围，判别方法具有更高的准确度和更简单的使用方法。

**知识要点**
* ML是计算机基于数据构建概率统计模型并运用模型对数据进行预测和分析的学科
* 根据输入输出类型不同，ML可以分为分类问题、回归问题、标注问题。
* 过拟合是ML中不可避免的问题
* 监督学习是目前ML中的主流，包括：生产方法和判别方法

## 2. 线性回归

线性模型的价值：可体现重要的思想，又能够构造出功能更加强大的非线性模型。

**基本概念**
* 基本思想：线性回归假设输出变量是若干输入变量的线性组合，并根据这一关系求解线性组合的最优系数
* 在ML中，回归问题隐含了输入变量和输出变量均可以连续取值的前提，因而**利用线性回归模型可以对任意输入给出对输出的估计**。
* 假定一个实例用$X=(x_1, x_2,...,x_n)$表示，线性回归就是求一组参数$w_i,i=0,1,...,n$, 使得预测输出可以表示为以这组参数为权重的实例属性的线性组合。如果引入常量$x_0=1$,线性回归试图学习的模型就是：
  
  $f(x)=W^TX=\sum\limits_{i=0}^nw_i \cdot x_i$

  当实例只有一个属性时，输入和输出的关系就是二维平面上的一条直线；当实例的数目较多时，线性回归得到的就是$n$维空间上的空间上的一个超平面，对应一个维度维$n-1$的线性子空间。
* 在训练集上确定系数$w_i$时，预测输出的$f(x)$和真实输出$y$之间的误差是关注的核心指标。在线性回归中，这一误差用**均方差**来定义。当模型为二维平面上的直线时，均方误差就是预测输出和真实输出之间的**欧氏距离**，也就是两点间向量的$L^2$范数。而以均方误差取得最小值为目标的模型求解方法就是**最小二乘法**,其表达式可以写为：

    ${\mathbf{w}}^* = \mathop {\arg \min }\limits_{\mathbf{w}} \sum\limits_{k = 1} {{{({{\mathbf{w}}^T}{{\mathbf{x}}_k} - {y_k})}^2}}$ 
    
    $= \mathop {\arg \min }\limits_{\mathbf{w}} \sum\limits_{k = 1} || y_k - \mathbf{w}^T \mathbf{x}_k ||^2$

    其中，$X_k$代表训练集中的一个样本。
* 在单变量线性回归任务中，最小二乘法的作用就是找到一条直线，使所有样本到直线的欧式距离之和最小。
* 


